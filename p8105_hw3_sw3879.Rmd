---
title: "p8105_hw3_sw3879"
author: "Siqing Wang"
date: "2023-10-12"
output: github_document
---

Setting up libraries
```{r setup, message = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(dplyr)
library(ggplot2)
```

## Problem 1

### Loadiing the dataset for Q1
```{r}
library(p8105.datasets)
data("instacart")

instacart = instacart |> as.tibble() |> janitor::clean_names()
```

### Describing the dataset 
The `instacart` dataset as `r nrow(instacart)` rows and `r ncol(instacart)` columns. Important variables include unique identifier of customer, order time, days since previous order, and specific information about the product purchased. There are a total of `r instacart |> select(order_id) |> n_distinct()` orders recorded, and `r instacart |> select(product_id) |> n_distinct()` recorded. These products range from `r instacart |> select(department_id) |> n_distinct()` different departments. 

### Answering questions 
```{r}
instacart |> count(aisle) |> arrange(desc(n))
```

There are `r instacart |> select(aisle_id) |> n_distinct()` aisles, and the most ordered items are from fresh vegetables and fresh fruits. 

Making a plot: 
```{r}
instacart |> count(aisle) |> filter(n > 10000) |> 
  ggplot(aes(x = reorder(aisle, -n), y = n)) + geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Number of product ordered by aisle",
       x = "Aisle Name", y = "Number of product ordered") 
```

Making a table of top 3 ordered items from "baking ingredients", "dog food care", "packaged vegetables fruits":
```{r}
instacart |> filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle) |> count(product_name) |> 
  arrange(desc(n)) |> top_n(n = 3, wt = n) |> 
  rename(times_ordered = n) |> 
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week: 
```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

## Problem 2

Load the dataset
```{r}
data("brfss_smart2010")
```

Clean data, rename variables, remove unnecessary observations, and reorder data. Reording is done by making response as a factor then use it to sort the observations. 
```{r}
brfss_clean = brfss_smart2010 |> as.tibble() |> 
  janitor::clean_names() |> 
  rename(location_abbr = locationabbr) |> 
  rename(location_desc = locationdesc) |> 
  filter(topic == "Overall Health") |> 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", 
                                                "Very good", "Excellent"), 
                           ordered = TRUE)) |> 
  arrange(response)
```

In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
result_2002 = brfss_clean |> filter(year == "2002") |> 
  group_by(location_abbr) |> 
  summarize(dictinct_loc_count = n_distinct(location_desc)) |> 
  filter(dictinct_loc_count >= 7)
```

```{r}
result_2010 = brfss_clean |> filter(year == "2010") |> 
  group_by(location_abbr) |> 
  summarize(dictinct_loc_count = n_distinct(location_desc)) |> 
  filter(dictinct_loc_count >= 7)
```

In 2002, `r nrow(result_2002)` states were observed at 7 or more locations, they are `r pull(result_2002, location_abbr)`

In 2010, `r nrow(result_2010)` states were observed at 7 or more locations, they are `r pull(result_2010, location_abbr)`

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state. This is done by first filtering for the excellent response, selecting out the necessary variables, then calcualte the average of data value grouped by year and date. 
```{r}
brfss_excellent = brfss_clean |> 
  filter(response == "Excellent") |> 
  select(c("year", "location_abbr", "data_value")) |> 
  group_by(year, location_abbr) |> 
  mutate(data_value = mean(data_value)) |> 
  distinct() |> 
  rename(state = location_abbr)
```
```{r}
brfss_excellent |> 
  ggplot(aes(x = year, y = data_value, group = state, color = as.factor(state))) +
  geom_line() +
  labs(title = "Average BRFSS data per state, 2002 - 2010",
       x = "Year", y = "Average BRFSS data", 
       color = "State")
```

This plot shows BRFSS data for all states from 2002 to 2010. The mean BRFSS in 2002 is `r brfss_excellent |> filter(year == "2002") |> pull(data_value) |> mean(na.rm = TRUE) |> round(digits = 2)`, and in 2010 is `r brfss_excellent |> filter(year == "2010") |> pull(data_value) |> mean(na.rm = TRUE) |> round(digits = 2)`. The state with the lowest average BRFSS data between 2002 and 2010 is `r brfss_excellent |> group_by(state) |> summarize(n = mean(data_value, na.rm = TRUE)) |> arrange(n) |> slice(1) |> select(2) |> round(digits = 2)` from the state `r brfss_excellent |> group_by(state) |> summarize(n = mean(data_value, na.rm = TRUE)) |> arrange(n) |> slice(1) |> select(1)`. The state with the highest average BRFSS data between 2002 and 2010 is `r brfss_excellent |> group_by(state) |> summarize(n = mean(data_value, na.rm = TRUE)) |> arrange(desc(n)) |> slice(1) |> select(2) |> round(digits = 2)` from the state `r brfss_excellent |> group_by(state) |> summarize(n = mean(data_value, na.rm = TRUE)) |> arrange(desc(n)) |> slice(1) |> select(1)`.

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State. This is done by filtering out rows for 2006 and 2000 and the NY states, then visualize data value by county and response.  
```{r}
brfss_ny = brfss_clean |> 
  filter(year %in% c("2006", "2010")) |> 
  filter(location_abbr == "NY") |> 
  select(c("year", "location_abbr", "location_desc","response", "data_value"))
```
```{r}
brfss_ny |> ggplot(aes(x = location_desc, y = data_value, fill = response)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Distribution of BRFSS data by response in NY",
       x = "County", y = "BRFSS data", fill = "Response") +
  facet_grid(. ~ year)
```

From the clustered bar plot, we can see that there are missing data in 2006 for a few counties but data is complete for 2010. In most states, the majority of the response is "very good", with Bronx, Kings, and Queens county as exceptions. Comparing between 2006 and 2010, there is generally an increase in data value in the "very good" response.

## Problem 3

Load the demographics dataset, recode variable according to csv, remove <21 years old, and remove records with any missing data 
```{r}
demo = read_csv("data/nhanes_covar.csv", skip = 4) |> 
  janitor::clean_names() |> 
  filter(age >= 21) |> 
  mutate(sex = case_match(
    sex, 1 ~ "Male", 2 ~ "Female"),
  sex = as.factor(sex)) |> 
  mutate(education = case_match(
    education, 
    1 ~ "Less than high school", 
    2 ~ "High school equivalent", 
    3 ~ "More than high school"
  ),
  education = factor(education,
                     levels = c("Less than high school", 
                                "High school equivalent",
                                "More than high school"))) |> 
  na.omit()
```

The demographics dataset has `r nrow(demo)` observations after clearning, and `r ncol(demo)` variables. `r 250 - nrow(demo)` observations were removed. Variables in this dataset includes `r colnames(demo) |> print()`. 

Load the accelerometer dataset, pivot to long format
```{r}
accel = read_csv("data/nhanes_accel.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    min1 : min1440,
    names_to = "time_min", 
    names_prefix = "min",
    values_to = "accel_data"
  )
```

Merge demo and accel data
```{r}
merged_df = left_join(demo, accel, by = "seqn") |> 
  mutate(time_min = as.numeric(time_min))
```

The merged data set has  `r nrow(merged_df)` rows and `r ncol(demo)` variables. There are a total of `r merged_df |> pull(seqn) |> n_distinct()` participants in the dataframe. This dataframe records the MIMS data for every minute in a day for these participants, along with their demographic information. 

Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category.
```{r}
demo |> group_by(education) |> count(sex) |> 
  pivot_wider(
    names_from = sex,
    values_from = n
  ) |> 
  rename(female_number = Female, male_number = Male) |> 
  knitr::kable()
```
```{r}
demo |> group_by(education, sex, age) |> 
  ggplot(aes(x = education, y = age, color = sex)) + geom_boxplot() + 
  labs(title = "Demographics by sex and education",
       x = "Education",
       y = "Age",
       color = "Sex")
```

From the table, we can see that the number of female and male with education "less than high school" and with "more than high school" are relatively similar, but more male with "high school equivalent" is included than female. From the box plot, we can see that the population with education more than high school is generally younger. The difference in age between female and male is the most significant in the high school equivalent education group. 

Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences.
```{r}
merged_df |> group_by(seqn) |> 
  summarize(aggre_data = sum(as.numeric(accel_data))) |> 
  right_join(demo) |> 
  ggplot(aes(x = age, y = aggre_data, color = sex)) + 
  geom_point() + geom_smooth(se = FALSE) +
  labs(title = "Total activity by age",
       x = "Age",
       y = "Total activity",
       color = "Sex") +
  facet_wrap(. ~ education)
```

We can see that there is a general downward trend in total activity as age increases. This is best evidenced in the less than high school education group, whereas decrease is slower in the more than high school group. In the high school equivalent education group, theere is a peak in activity at ~age = 40. In high school equivalent and more than high school education group, female generally has higher activity level, whereas in the less than high school education group there is no such difference. 

Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. 
```{r}
merged_df |> 
  group_by(time_min, education, sex) |> 
  summarize(data_per_min = mean(as.numeric(accel_data))) |> 
  ggplot(aes(x = time_min, y = data_per_min, color = sex)) +
  geom_point(alpha = 0.05) + geom_smooth(se = FALSE) +
  labs(title = "Total activity by minutes in a day",
       x = "Time in min within a day",
       y = "Total activity per min",
       color = "Sex") +
  facet_wrap(. ~ education)
```

Looking at the three panes, we observe similar trends of activity at different time points within a day, with lowest activity in midnight, and highest during middle of the day. The lowest point is at around 0-3, and the highest point is at around 12-15, Similarly, in high school equivalent and more than high school education group, female generally has higher activity level than men, especially at the peak level. 


