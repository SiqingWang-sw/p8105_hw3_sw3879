---
title: "p8105_hw3_sw3879"
author: "Siqing Wang"
date: "2023-10-12"
output: github_document
---

Setting up libraries
```{r setup, message = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(dplyr)
library(ggplot2)
```

## Problem 1

### Loadiing the dataset for Q1
```{r}
library(p8105.datasets)
data("instacart")

instacart = instacart |> as.tibble() |> janitor::clean_names()
```

### Describing the dataset 
The `instacart` dataset as `r nrow(instacart)` rows and `r ncol(instacart)` columns. Important variables include unique identifier of customer, order date, days since previous order, and specific information about the product purchased. There are a total of `r instacart |> select(order_id) |> n_distinct()` orders recorded, and `r instacart |> select(product_id) |> n_distinct()` recorded. These products range from `r instacart |> select(department_id) |> n_distinct()` different departments. 

### Answering questions 
```{r}
instacart |> count(aisle) |> arrange(desc(n))
```

There are `r instacart |> select(aisle_id) |> n_distinct()` aisles, and the most ordered items are from fresh vegetables and fresh fruits. 

Making a plot: 
```{r}
instacart |> count(aisle) |> filter(n > 10000) |> 
  ggplot(aes(x = reorder(aisle, -n), y = n)) + geom_point() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Number of product ordered by aisle",
       x = "Aisle Name", y = "Number of product ordered") 
```

Making a table of top 3 ordered items from "baking ingredients", "dog food care", "packaged vegetables fruits":
```{r}
instacart |> filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle) |> count(product_name) |> 
  arrange(desc(n)) |> top_n(n = 3, wt = n) |> 
  rename(times_ordered = n) |> 
  knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week: 
```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  pivot_wider(
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

## Problem 2

Load the dataset
```{r}
data("brfss_smart2010")
```

Clean data, rename variables, remove unnecessary observations, and reorder data
```{r}
brfss_clean = brfss_smart2010 |> as.tibble() |> 
  janitor::clean_names() |> 
  rename(location_abbr = locationabbr) |> 
  rename(location_desc = locationdesc) |> 
  filter(topic == "Overall Health") |> 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", 
                                                "Very good", "Excellent"), 
                           ordered = TRUE)) |> 
  arrange(response)
```

In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
result_2002 = brfss_clean |> filter(year == "2002") |> 
  group_by(location_abbr) |> 
  summarize(dictinct_loc_count = n_distinct(location_desc)) |> 
  filter(dictinct_loc_count >= 7)
```

```{r}
result_2010 = brfss_clean |> filter(year == "2010") |> 
  group_by(location_abbr) |> 
  summarize(dictinct_loc_count = n_distinct(location_desc)) |> 
  filter(dictinct_loc_count >= 7)
```

In 2002, `r nrow(result_2002)` states were observed at 7 or more locations, they are `r pull(result_2002, location_abbr)`

In 2010, `r nrow(result_2010)` states were observed at 7 or more locations, they are `r pull(result_2010, location_abbr)`

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state
```{r}
brfss_excellent = brfss_clean |> 
  filter(response == "Excellent") |> 
  select(c("year", "location_abbr", "data_value")) |> 
  group_by(year, location_abbr) |> 
  mutate(data_value = mean(data_value)) |> 
  distinct() |> 
  rename(state = location_abbr)
```
```{r}
brfss_excellent |> 
  ggplot(aes(x = year, y = data_value, group = state, color = as.factor(state))) +
  geom_line() +
  labs(title = "Average BRFSS data per state, 2002 - 2010",
       x = "Year", y = "Average BRFSS data", 
       color = "State")
```

TO DO: comment on result

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r}
brfss_ny = brfss_clean |> 
  filter(year %in% c("2006", "2010")) |> 
  filter(location_abbr == "NY") |> 
  select(c("year", "location_abbr", "location_desc","response", "data_value"))
```
```{r}
brfss_ny |> ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Distribution of BRFSS data by response in NY",
       x = "Response", y = "BRFSS data") +
  facet_grid(. ~ year)
```

TO DO: describe data 

## Problem 3

Load the demographics dataset, recode variable and remove <21 years old
```{r}
demo = read_csv("data/nhanes_covar.csv", skip = 4) |> 
  janitor::clean_names() |> 
  filter(age > 21) |> 
  mutate(sex = case_match(
    sex, 1 ~ "Male", 2 ~ "Female"
  ),
  sex = as.factor(sex)) |> 
  mutate(education = case_match(
    education, 
    1 ~ "Less than high school", 
    2 ~ "High school equivalent", 
    3 ~ "More than high school"
  ),
  education = as.factor(education))
```

Load the accelerometer dataset, pivot to long format
```{r}
accel = read_csv("data/nhanes_accel.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    min1 : min1440,
    names_to = "time_min", 
    names_prefix = "min",
    values_to = "accel_data"
  )
```

Merge demo and accel data
```{r}
merged_df = left_join(demo, accel, by = "seqn")
```






